{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "oe4CIvjDQgSh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "TL_yZbWMQ8p_",
    "outputId": "138acc86-e6ec-4731-b2d1-c929fa95b631"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f386ed7c-d888-4c49-94a6-24aab4874c8c",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni..."
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say..."
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"SMSSpamCollection\", sep=\"\\t\", names=[\"label\",\"text\"])\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "textdata = df[\"text\"]\n",
    "label = df[\"label\"]\n",
    "N = len(textdata)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ca5bc0f"
   },
   "source": [
    "# Preprocessing\n",
    "Preprocess the text data in the dataframe `df` by cleaning, tokenizing, removing stop words, lemmatizing, and vectorizing it. Then, split the preprocessed data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "zUiWicz7WmXW",
    "outputId": "2912468a-3ffe-4ae5-b4e3-5027f6897fb0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e36c54e6-ae23-404e-afcf-a1563d9197c5",
       "rows": [
        [
         "0",
         "go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat"
        ],
        [
         "1",
         "ok lar joking wif u oni"
        ],
        [
         "2",
         "free entry in  a wkly comp to win fa cup final tkts st may  text fa to  to receive entry questionstd txt ratetcs apply overs"
        ],
        [
         "3",
         "u dun say so early hor u c already then say"
        ],
        [
         "4",
         "nah i dont think he goes to usf he lives around here though"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/plain": [
       "0    go until jurong point crazy available only in ...\n",
       "1                              ok lar joking wif u oni\n",
       "2    free entry in  a wkly comp to win fa cup final...\n",
       "3          u dun say so early hor u c already then say\n",
       "4    nah i dont think he goes to usf he lives aroun...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(N):\n",
    "  textdata[i] = textdata[i].lower()\n",
    "\n",
    "def cleanTexts(text):\n",
    "  cleanText = re.sub(r'[^a-z\\s]', '', text)\n",
    "  return cleanText\n",
    "\n",
    "textdata = textdata.apply(cleanTexts)\n",
    "\n",
    "textdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHpR9C8kvmUs"
   },
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "id": "RVsyELWPvbxk"
   },
   "outputs": [],
   "source": [
    "def getVocab(textdata):\n",
    "  vocab = {}\n",
    "  i = 0\n",
    "  for txt in textdata:\n",
    "      for token in re.findall(r\"\\b[a-z]+\\b\", txt):\n",
    "          if len(token) > 1 and token not in stopwords and token not in vocab:\n",
    "                vocab[token] = i\n",
    "                i += 1\n",
    "  return vocab\n",
    "\n",
    "def vectorize(text,vocab):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    for token in re.findall(r\"\\b[a-z]+\\b\", text):\n",
    "        if token in vocab:\n",
    "            index = vocab[token]\n",
    "            vector[index] = 1\n",
    "    return vector\n",
    "\n",
    "vocab = getVocab(textdata)\n",
    "\n",
    "def splitDataset(X,split:float):\n",
    "    N = len(X)\n",
    "    n = int(split*N)\n",
    "    X_train = np.stack(X[:n].apply(vectorize, vocab=vocab))\n",
    "    Y_train = np.where(label[:n] == \"spam\", 1, 0)\n",
    "\n",
    "    X_test = np.stack(X[n:].apply(vectorize, vocab=vocab))\n",
    "    Y_test = np.where(label[n:] == \"spam\", 1, 0)\n",
    "    \n",
    "    return (X_train,Y_train),(X_test,Y_test)\n",
    "\n",
    "(X_train,Y_train),(X_test,Y_test) = splitDataset(textdata,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8usfUPTj2On-"
   },
   "source": [
    "## Calculating priors and likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "IhnkIESr2N7s"
   },
   "outputs": [],
   "source": [
    "def trainModel(X_train, Y_train, classes: tuple):\n",
    "    n, d = X_train.shape\n",
    "    model = {}\n",
    "    for c in classes:\n",
    "        x_c = X_train[Y_train == c]\n",
    "        n_c = x_c.shape[0]\n",
    "        prior = n_c/n\n",
    "\n",
    "        wordcnt = np.sum(x_c, axis=0)\n",
    "\n",
    "        ProbWordsinc = (wordcnt +0.5) / (n_c + d)\n",
    "        model[c] = (prior,ProbWordsinc)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predictClass(x_test, model):\n",
    "    log_posteriors = {}\n",
    "    for c, params in model.items():\n",
    "        prior = params[0]\n",
    "        wordProbs = params[1]\n",
    "\n",
    "        log_prob = np.log(prior)\n",
    "\n",
    "        log_prob += np.sum(x_test * np.log(wordProbs) + (1 - x_test) * np.log(1 - wordProbs))\n",
    "\n",
    "        log_posteriors[c] = log_prob\n",
    "    return max(log_posteriors, key=log_posteriors.get) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a23cbf5",
    "outputId": "35f06311-a036-4119-c990-df802ed6325e"
   },
   "outputs": [],
   "source": [
    "model = trainModel(X_train, Y_train, classes=(0,1))\n",
    "\n",
    "predictions = []\n",
    "for x_test in X_test:\n",
    "  prediction = predictClass(x_test, model)\n",
    "  predictions.append(prediction)\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "TP: 66, FP: 6\n",
      "FN: 6, TN: 480\n",
      "Accuracy: 97.84946236559139\n",
      "Recall: 91.66666666666666\n",
      "Precision: 91.66666666666666\n",
      "F1 Score: 91.66666666666666\n"
     ]
    }
   ],
   "source": [
    "def ConfusionMatrix(Y_test, predictions):\n",
    "    TP = np.sum((Y_test == 1) & (predictions == 1))\n",
    "    TN = np.sum((Y_test == 0) & (predictions == 0))\n",
    "    FP = np.sum((Y_test == 0) & (predictions == 1))\n",
    "    FN = np.sum((Y_test == 1) & (predictions == 0))\n",
    "\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    R = TP / (TP + FN)\n",
    "    P = TP / (TP + FP)\n",
    "    F1 = 2 * (P * R) / (P + R)\n",
    "    print(f\"Confusion Matrix:\\nTP: {TP}, FP: {FP}\\nFN: {FN}, TN: {TN}\")\n",
    "    print(f\"Accuracy: {accuracy*100}\")\n",
    "    print(f\"Recall: {R*100}\")\n",
    "    print(f\"Precision: {P*100}\")\n",
    "    print(f\"F1 Score: {F1*100}\")\n",
    "    return ([[TP, FP], [FN, TN]], accuracy, R, P, F1)\n",
    "\n",
    "evaluation = ConfusionMatrix(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's a HAM\n"
     ]
    }
   ],
   "source": [
    "def testModel(text):\n",
    "    text = vectorize(cleanTexts(text),vocab)\n",
    "    return \"it's a SPAM!!\" if predictClass(text,model) else \"it's a HAM\"\n",
    "\n",
    "print(testModel(\"See you tomorrow at lunch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiClass Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "classes = (\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\")\n",
    "\n",
    "\n",
    "def load_bbc_dataset(base_path):\n",
    "    data = {}\n",
    "    for category in os.listdir(base_path):\n",
    "        texts = []\n",
    "        labels = []\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            for file_name in os.listdir(category_path):\n",
    "                file_path = os.path.join(category_path, file_name)\n",
    "                with open(file_path, 'r', encoding='latin1') as f:\n",
    "                    text = f.read()\n",
    "                    texts.append(text)\n",
    "                    labels.append(category)\n",
    "        data[f\"{category}\"] = pd.DataFrame({'text': texts, 'label': labels})\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "base_path = 'BBCNewsSummary/NewsArticles'\n",
    "bbc = load_bbc_dataset(base_path)\n",
    "business = bbc[\"business\"]\n",
    "entertainment = bbc[\"entertainment\"]\n",
    "politics = bbc[\"politics\"]\n",
    "sport = bbc[\"sport\"]\n",
    "tech = bbc[\"tech\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.9\n",
    "bbc_train = pd.concat([business[:int(len(business) * split)], \n",
    "                       entertainment[:int(len(entertainment) * split)], \n",
    "                       politics[:int(len(politics) * split)], \n",
    "                       sport[:int(len(sport) * split)], \n",
    "                       tech[:int(len(tech) * split)]], ignore_index=True)\n",
    "bbc_test = pd.concat([business[int(len(business) * split):], \n",
    "                       entertainment[int(len(entertainment) * split):], \n",
    "                       politics[int(len(politics) * split):], \n",
    "                       sport[int(len(sport) * split):], \n",
    "                       tech[int(len(tech) * split):]], ignore_index=True)\n",
    "\n",
    "bbcX_train = bbc_train[\"text\"].apply(cleanTexts)\n",
    "bbcY_train = bbc_train[\"label\"]\n",
    "bbcX_test = bbc_test[\"text\"].apply(cleanTexts)\n",
    "bbcY_test = bbc_test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_vocab = getVocab(bbcX_train)\n",
    "\n",
    "def vectorize(text,vocab):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    for token in re.findall(r\"\\b[a-z]+\\b\", text):\n",
    "        if token in vocab:\n",
    "            index = vocab[token]\n",
    "            vector[index] = 1\n",
    "    return vector\n",
    "\n",
    "bbcX_train = np.stack(bbcX_train.apply(vectorize, vocab=bbc_vocab)) # type: ignore\n",
    "bbcY_train = np.array([classes.index(label) for label in bbcY_train])\n",
    "bbcX_test = np.stack(bbcX_test.apply(vectorize, vocab=bbc_vocab)) # type: ignore\n",
    "bbcY_test = np.array([classes.index(label) for label in bbcY_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_model = trainModel(bbcX_train, bbcY_train, classes=tuple(range(len(classes))))\n",
    "\n",
    "bbc_predictions = []\n",
    "for x_test in bbcX_test:\n",
    "  prediction = predictClass(x_test, bbc_model)\n",
    "  bbc_predictions.append(prediction)\n",
    "\n",
    "bbc_predictions = np.array(bbc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 96.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "business",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "entertainment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "politics",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sport",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tech",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ab019d24-95d4-40a7-b3e9-22ad7ce5b0fc",
       "rows": [
        [
         "business",
         "50",
         "0",
         "1",
         "0",
         "0"
        ],
        [
         "entertainment",
         "0",
         "35",
         "2",
         "0",
         "2"
        ],
        [
         "politics",
         "0",
         "0",
         "41",
         "0",
         "1"
        ],
        [
         "sport",
         "1",
         "0",
         "1",
         "50",
         "0"
        ],
        [
         "tech",
         "1",
         "0",
         "0",
         "0",
         "40"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>politics</th>\n",
       "      <th>sport</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sport</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tech</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               business  entertainment  politics  sport  tech\n",
       "business             50              0         1      0     0\n",
       "entertainment         0             35         2      0     2\n",
       "politics              0              0        41      0     1\n",
       "sport                 1              0         1     50     0\n",
       "tech                  1              0         0      0    40"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bbc_confusionMatrix(Y_test, predictions):\n",
    "\tmatrix = [[np.sum((Y_test == 0) & (predictions == 0)), np.sum((Y_test == 0) & (predictions == 1)),np.sum((Y_test == 0) & (predictions == 2)),np.sum((Y_test == 0) & (predictions == 3)),np.sum((Y_test == 0) & (predictions == 4))],\n",
    "\t\t\t[np.sum((Y_test == 1) & (predictions == 0)), np.sum((Y_test == 1) & (predictions == 1)),np.sum((Y_test == 1) & (predictions == 2)),np.sum((Y_test == 1) & (predictions == 3)),np.sum((Y_test == 1) & (predictions == 4))],\n",
    "\t\t\t[np.sum((Y_test == 2) & (predictions == 0)), np.sum((Y_test == 2) & (predictions == 1)),np.sum((Y_test == 2) & (predictions == 2)),np.sum((Y_test == 2) & (predictions == 3)),np.sum((Y_test == 2) & (predictions == 4))],\n",
    "\t\t\t[np.sum((Y_test == 3) & (predictions == 0)), np.sum((Y_test == 3) & (predictions == 1)),np.sum((Y_test == 3) & (predictions == 2)),np.sum((Y_test == 3) & (predictions == 3)),np.sum((Y_test == 3) & (predictions == 4))],\n",
    "\t\t\t[np.sum((Y_test == 4) & (predictions == 0)), np.sum((Y_test == 4) & (predictions == 1)),np.sum((Y_test == 4) & (predictions == 2)),np.sum((Y_test == 4) & (predictions == 3)),np.sum((Y_test == 4) & (predictions == 4))]]\n",
    "\t\n",
    "\taccuracy = np.sum(np.diag(matrix)) / np.sum(matrix)\n",
    "\tdf = pd.DataFrame(matrix, index=classes, columns=classes)\n",
    "\treturn (df, accuracy)\n",
    " \n",
    "df, acc = bbc_confusionMatrix(bbcY_test, bbc_predictions)\n",
    "print(\"accuracy:\", acc*100)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it's a business article\""
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def testBBCModel(text):\n",
    "\ttext = vectorize(cleanTexts(text),bbc_vocab)\n",
    "\treturn f\"it's a {classes[predictClass(text,bbc_model)]} article\"\n",
    "\n",
    "testBBCModel(\"news article about sports\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
